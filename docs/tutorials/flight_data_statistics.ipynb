{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating MPE and Qual statistical levels from flight data\n",
    "\n",
    "This and other notebooks are available here: https://github.com/twmacro/pyyeti/tree/master/docs/tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares two methods to compute \"MPE\" (maximum predicted environment) and \"Qual\" levels from simulated flight data. The simulated flight data could be for a shock response spectrum (SRS) level at some frequency. In this notebook, MPE is the \"P95/50\" level and Qual is the \"P99/90\" level. In other words, the MPE level will have a 50% chance of bounding 95% of the population and the Qual level will have a 90% chance of bounding 99% of the population. The first number (95% for example) can be called the normal probability limit, and the second number (50% for example) can be called the confidence level. The equations for both methods are related to each other and are derived below following Owen [1].\n",
    "\n",
    "The first method is to assume that the standard deviation of the population is 3 dB. This has been used in MIL-STD-1540 and is still maintained in \"SMC Standard SMC-S-016\" [2]. It is further supported by Yunis [3]. Method 1 will use the equation:\n",
    "\n",
    "$$Level_1 = \\bar{x}_g + (Z_p + \\frac{Z_c}{\\sqrt{n}})\\, \\sigma$$\n",
    "\n",
    "where $\\bar{x}_g$ is the sample geometric mean (see below), $Z_p$ is the number standard deviations to go from the mean to the normal probability limit, $Z_c$ is the number standard deviations to go from the mean to the confidence level, and $n$ is the number of samples. For a P95/50 level, if $F_N$ is the cumulative distribution function for the normal distribution:\n",
    "\n",
    "$$P_p = 0.95$$\n",
    "$$Z_p = F_N^{-1}(P_p) = \\text{scipy.stats.norm.ppf}(P_p) = 1.64$$\n",
    "\n",
    "$$P_c = 0.50$$\n",
    "$$Z_c = F_N^{-1}(P_c) = \\text{scipy.stats.norm.ppf}(P_c) = 0.0$$\n",
    "\n",
    "where $P_p$ is the normal probability level and $P_c$ is the confidence level.\n",
    "\n",
    "The second method is to use normal tolerance limits to compute the statistical bounds. Specifically, we'll use the function ``pyyeti.stats.ksingle``. We'll show that the two methods can be equivalent if two conditions are met: the simulated flight sample standard deviation is 3 dB (as assumed in the first method), and we have enough flights for an accurate estimate of the standard deviation. We'll also explore what happens for fewer flights. This has been explored previously [3]. Method 2 will use the noncentral *t*-distribution. If $F_T$ is the cumulative distribution function of the noncentral *t*-distribution, then:\n",
    "\n",
    "$$Level_2 = \\bar{x}_g + \\frac{F_T^{-1}(P_c, n-1, Z_p \\sqrt{n})}{\\sqrt{n}}\\, s = \\bar{x}_g + \\text{pyyeti.stats.ksingle}(P_p, P_c, n)\\, s$$\n",
    "\n",
    "where $s$ is the sample standard deviation in dB, and the value $Z_p \\sqrt{n}$ is the noncentrality parameter.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[1] D. B. Owen, \"Factors for One-Sided Tolerance Limits and for Variables Sampling Plans\", SCR-607, Mathematics and Computers, TID-4500 (19th Edition), Sandia Corporation Monograph, March 1963.\n",
    "\n",
    "[2] SMC-S-016 (2014), Air Force Space Command, Space and Missile Systems Center Standard, \"Test Requirements for Launch, Upper-Stage, and Space Vehicles (05-SEP-2014) Superseding SMC-S-016 (2008).\n",
    "\n",
    "[3] Isam Yunis, \"Standard Deviation of Launch Vehicle Vibration and Acoustic Environments\", Journal of Spacecraft and Rockets, Vol. 50, No. 4, Julyâ€“August 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a dB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for computing how many dB's it is from $value_1$ to $value_2$ is:\n",
    "\n",
    "$$d = 20 \\log_{10}(\\frac{value_2}{value_1}) \\, \\text{dB}$$\n",
    "\n",
    "Rearranging that equation, we have:\n",
    "\n",
    "$$value_2 = value_1 \\cdot 10^{d/20}$$\n",
    "    \n",
    "Therefore, when $d$ dB is \"added\" to a number, it really means to multiply the number by $10^{d/20}$. For example, to add $6 \\, \\text{dB}$ is to approximately double a value:\n",
    "\n",
    "$$value_2 = value_1 + 6 \\, \\text{dB} = value_1 \\cdot 10^{6/20} \\approx 2 \\cdot value_1$$\n",
    "\n",
    "#### Aside\n",
    "\n",
    "It is noted that for squared quantities $value_1$ and $value_2$ (as in \"power\" units), the 20 becomes 10 because the power of 2 is inside the log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean to specify the standard deviation in dB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the standard deviation is specified in dB, it really means that the data is assumed to follow a log-normal distribution. The following shows how this math works.\n",
    "\n",
    "Let's look at the calculation of an \"$k \\sigma$\" level (where $\\mu$ is the arithmetic mean) of normally distributed data:\n",
    "\n",
    "$$level = \\mu + k \\sigma$$\n",
    "\n",
    "If $\\sigma$ is in dB, then, as we saw above, that equation is really:\n",
    "\n",
    "$$level = \\mu_g \\cdot 10^{k \\sigma / 20}$$\n",
    "\n",
    "We're using $\\mu_g$ instead of $\\mu$ to denote that this is the geometric mean (as we'll show below).\n",
    "\n",
    "So, if $k$ is 2 and $\\sigma$ is 3 dB, then the 2-sigma level is approximately twice the mean. Clearly, this is not a normal distribution where $k \\sigma$ would be *added* to the mean. However, multiplication becomes addition in log space. So, to put in a more standard form, take the logarithm of both sides of the previous equation:\n",
    "\n",
    "$$\\log_{10}(level) = \\log_{10}(\\mu_g \\cdot 10^{k \\sigma / 20})$$\n",
    "\n",
    "$$\\log_{10}(level) = \\log_{10}(\\mu_g) + \\log_{10}(10^{k \\sigma / 20})$$\n",
    "\n",
    "$$\\log_{10}(level) = \\log_{10}(\\mu_g) + k \\sigma / 20$$\n",
    "\n",
    "$$\\log_{10}(level) = \\log_{10}(\\mu_g) + k \\sigma_{log_{10}}$$\n",
    "\n",
    "Therefore, the standard deviation of the log10 of the data ($\\sigma_{log_{10}}$) is related to standard deviation in dB ($\\sigma$) by:\n",
    "\n",
    "$$\\sigma_{log_{10}} = \\sigma / 20$$\n",
    "\n",
    "(Note: the natural logarithm could have been used as well. In that case: $\\sigma_{log_e} = \\sigma / 20 \\log_e(10)$. This fact will be used below to generate log-normal random deviates with ``scipy.stats.lognorm.rvs``.)\n",
    "\n",
    "Consider the \"mean\" term. If the data are assumed to follow the log-normal distribution, then that term is the mean of the log of the data ($\\mu_{log_{10}}$):\n",
    "\n",
    "$$\\log_{10}(\\mu_g) = \\mu_{log_{10}} = \\frac{\\log_{10}(x_1) + \\log_{10}(x_2) ... + \\log_{10}(x_n)}{n} = \\log_{10}((x_1 \\cdot x_2 \\cdot ... x_n)^{1/n})$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\mu_g = (x_1 \\cdot x_2 \\cdot ... x_n)^{1/n}$$\n",
    "\n",
    "So $\\mu_g$ is the geometric mean of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical background on single sided tolerance limits for normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to calculate a limit that bounds some portion of the population with some level of confidence. If we knew the population mean and standard deviation, the limit could be calculated easily with 100% confidence:\n",
    "\n",
    "$$limit = \\mu + Z_p \\sigma$$\n",
    "\n",
    "In our case however, we do not know these values, so we wish to find a value $k$ such that:\n",
    "\n",
    "$$Pr\\{\\bar{x} + k \\, s \\geq \\mu + Z_p \\sigma\\} = P_c~~~~~~~~~~~~~~~~~~(1)$$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean and $s$ is the sample standard deviation.\n",
    "\n",
    "It is known that the sample mean follows the normal distribution $\\mathcal{N}(\\mu, \\sigma^2 / n)$. It is also known that $(n-1)s^2/\\sigma^2$ follows a Chi-square distribution with degrees-of-freedom $n-1$:\n",
    "\n",
    "$$(n-1)s^2/\\sigma^2 \\sim \\chi^2_{n-1}$$\n",
    "\n",
    "Rearranging equation (1):\n",
    "\n",
    "$$Pr\\left \\{\\frac{\\bar{x}-\\mu}{\\sigma} + \\frac{k \\, s}{\\sigma} \\geq Z_p \\right \\} = P_c$$\n",
    "\n",
    "$$Pr\\left \\{\\frac{\\mu - \\bar{x}}{\\sigma} + Z_p \\leq \\frac{k \\, s}{\\sigma}\\right \\} = P_c$$\n",
    "\n",
    "$$Pr\\left \\{\\frac{\\mu - \\bar{x}}{\\frac{\\sigma}{\\sqrt{n}}} + Z_p \\sqrt{n} \\leq \\frac{k \\, s}{\\sigma} \\sqrt{n}\\right \\} = P_c$$\n",
    "\n",
    "$$Pr\\left \\{\n",
    "        \\frac{\\frac{\\mu - \\bar{x}}{\\sigma / \\sqrt{n}} + Z_p \\sqrt{n}}\n",
    "             {s /\\sigma}\n",
    "           \\leq k \\sqrt{n} \\right \\} = P_c~~~~~~~~~~~~~~~~~~(2)$$\n",
    "\n",
    "The form of equation (2) is desirable because it is in the form of the noncentral *t*-distribution [4]. If $G$ is a normally distributed random variable with unit variance and zero mean, and $V$ is a Chi-squared distributed random variable with $n-1$ degrees of freedom that is independent of $G$, then:\n",
    "\n",
    "$$T = \\frac{G + \\delta}{\\sqrt{V / (n-1)}}$$\n",
    "\n",
    "is a noncentral *t*-distributed random variable with $n-1$ degrees of freedom and noncentrality parameter $\\delta \\neq 0$. Note that the noncentrality parameter may be negative. In our case:\n",
    "\n",
    "$$G = \\frac{\\mu - \\bar{x}}{\\sigma / \\sqrt{n}}$$\n",
    "\n",
    "$$\\delta = Z_p \\sqrt{n}$$\n",
    "\n",
    "$$V = (n-1)s^2/\\sigma^2$$\n",
    "\n",
    "Reference:\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Noncentral_t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the k factor for method 1:\n",
    "\n",
    "In the case where we assume $\\sigma$ is 3 dB, then we do not compute $s$ from the sample and just set $s = \\sigma$. Equation (2) becomes:\n",
    "\n",
    "$$Pr\\left \\{\\frac{\\mu - \\bar{x}}{\\sigma / \\sqrt{n}} \\leq (k - Z_p) \\sqrt{n} \\right \\} = P_c$$\n",
    "\n",
    "The first fraction in that equation follows the standard normal distribution: $\\mathcal{N}(0, 1)$. Noting that $Pr\\left \\{ \\mathcal{N}(0, 1) \\leq Z_c \\right \\} = P_c$, we have:\n",
    "\n",
    "$$(k - Z_p) \\sqrt{n} = Z_c$$\n",
    "\n",
    "$$k = Z_p + Z_c / \\sqrt{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the k factor for method 2:\n",
    "\n",
    "To find the $k$ factor for method 2, we do not assume we know $\\sigma$. In that case, equation (2) is in the form of the noncentral *t*-distribution and can be written:\n",
    "\n",
    "$$Pr\\left \\{T_{n-1,\\,\\delta =Z_p \\sqrt{n}} \\leq k \\sqrt{n} \\right \\} = P_c$$\n",
    "\n",
    "where $T_{n-1,\\,\\delta =Z_p \\sqrt{n}}$ is a noncentral *t* random variable with $n-1$ degrees-of-freedom and the noncentral parameter $Z_p \\sqrt{n}$. Solving for $k$ gives the result shown above:\n",
    "\n",
    "$$k = \\frac{F_T^{-1}(P_c, n-1, Z_p \\sqrt{n})}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some settings specifically for the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, lognorm, chi2\n",
    "import pandas as pd\n",
    "import pyyeti.stats\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 3.5]\n",
    "plt.rcParams['figure.dpi'] = 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Testing sample mean and variance distributions\n",
    "\n",
    "When sampling a normally distributed random variable $\\mathcal{N}(\\mu, \\sigma^2)$ , it was stated above that the sample mean follows the distribution:\n",
    "\n",
    "$$\\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma^2 / n)$$\n",
    "\n",
    "and that $(n-1)s^2/\\sigma^2$ follows a Chi-square distribution with degrees-of-freedom $n-1$:\n",
    "\n",
    "$$(n-1)s^2/\\sigma^2 \\sim \\chi^2_{n-1}$$\n",
    "\n",
    "This will be demonstrated in the following cell where the theory is compared to the results from a simple simulation:\n",
    "\n",
    "1. Repeat the following `n_simulations` times:\n",
    "\n",
    "    1. Generate `sample_size` pseudo random deviates that follow a normal distribution\n",
    "    2. Compute the sample mean and sample variance\n",
    "\n",
    "2. Plot a histograms of the results and compare to theoretical distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 6.0\n",
    "n = 9\n",
    "mu = 1.0\n",
    "df = n - 1\n",
    "\n",
    "n_simulations = 3000\n",
    "sample_size = n\n",
    "\n",
    "mean_s = np.empty(n_simulations)\n",
    "var_s = np.empty(n_simulations)\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    x = np.random.randn(sample_size) * np.sqrt(var) + mu\n",
    "    mean_s[i] = x.mean()\n",
    "    var_s[i] = x.var(ddof=1)\n",
    "\n",
    "# for plot:\n",
    "N = 100\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1, 1)\n",
    "\n",
    "# plot theoretical mean distribution:\n",
    "scale = np.sqrt(var / n)\n",
    "x = np.linspace(\n",
    "    norm.ppf(0.001, loc=mu, scale=scale), norm.ppf(0.999, loc=mu, scale=scale), N\n",
    ")\n",
    "lines = []  # keep track of handles so legend is in desired order\n",
    "lines += ax.plot(\n",
    "    x, norm.pdf(x, loc=mu, scale=scale), label=\"sample mean pdf from theory\"\n",
    ")\n",
    "\n",
    "# plot simulation mean distribution:\n",
    "lines += ax.hist(\n",
    "    mean_s,\n",
    "    density=True,\n",
    "    histtype=\"stepfilled\",\n",
    "    bins=15,\n",
    "    alpha=0.6,\n",
    "    label=\"sample mean pdf from simulation\",\n",
    ")[2]\n",
    "\n",
    "# plot theoretical variance distribution:\n",
    "scale = var / df\n",
    "x = np.linspace(chi2.ppf(0.001, df, scale=scale), chi2.ppf(0.999, df, scale=scale), N)\n",
    "lines += ax.plot(\n",
    "    x, chi2.pdf(x, df, scale=scale), label=\"sample variance pdf from theory\"\n",
    ")\n",
    "\n",
    "# plot simulation variance distribution:\n",
    "lines += ax.hist(\n",
    "    var_s,\n",
    "    density=True,\n",
    "    histtype=\"stepfilled\",\n",
    "    bins=15,\n",
    "    alpha=0.6,\n",
    "    label=\"sample variance pdf from simulation\",\n",
    ")[2]\n",
    "ax.legend(handles=lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Show that both methods yield same k-factor with enough samples\n",
    "\n",
    "As the number of samples increase, confidence approaches 100%. This means that the k-factor should approach $Z_p$.\n",
    "\n",
    "To begin, define desired statistical levels. These will be used below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {\n",
    "    \"Mean\": (0.50, 0.50),\n",
    "    \"MPE\": (0.95, 0.50),\n",
    "    \"Qual\": (0.99, 0.90),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.5))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "n = np.round(np.logspace(1, 6, 100)).astype(int)\n",
    "for ax, (level, (p, c)) in zip(axes, levels.items()):\n",
    "    zp = norm.ppf(p)\n",
    "    zc = norm.ppf(c)\n",
    "    ax.semilogx(n, zp + zc / np.sqrt(n), label=\"zp + zc / np.sqrt(n)\")\n",
    "    ax.semilogx(n, pyyeti.stats.ksingle(p, c, n), label=\"ksingle\")\n",
    "    ax.axhline(zp, color=\"black\", alpha=0.3, linewidth=4, label=\"Exact\", zorder=-1)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{level} (P{p*100:g}/{c*100:g})\")\n",
    "    ax.set_ylabel(\"k factor\")\n",
    "    ax.set_xlabel(\"Number of Samples\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: How many samples are needed for estimating MPE and Qual levels from flight data?\n",
    "\n",
    "This exercise is not difficult, but there are a number of steps involved. After running an earlier version of this notebook which only used $\\sigma = 3 \\text{dB}$, it was decided to also look at $\\sigma = 1.5 \\text{dB}$. Here is an outline:\n",
    "\n",
    "1. Define analysis parameters: the geometric mean, sample sizes, the number of simulations, and the assumed standard deviations in dB. A 10,000 sample size is included to confirm that the simulations approach the \"exact\" value if enough samples are included.\n",
    "\n",
    "2. Generate a pandas DataFrame to store simulation results. Care was taken to have nice plot labels:\n",
    "\n",
    "    1. \"Set Ïƒ\" means using method 1 above, where $\\sigma$ is assumed known and $k = Z_p + Z_c / \\sqrt{n}$\n",
    "    2. \"NTL\" means using method 2 above, where $k$ is the normal tolerance limit factor computed by using the noncentral *t*-distribution (``pyyeti.stats.ksingle``).\n",
    "    \n",
    "3. Run simulation to fill DataFrame with results. Log normal random deviates are generated to simulate flight data, using the parameters defined above. Then, both methods are used to estimate MPE (P95/50) and Qual (P99/90) levels. The mean of the estimate and the standard deviation of the estimate are stored in the DataFrame for plotting the results with error bars (in steps 5 and 6). The error bars will show the standard deviation of the estimate: large error bars indicate that more samples may be needed. For example, if there are 1000 simulations to compute the MPE level, the bar height is set to the mean of the 1000 estimates, and the error bars are set to the standard deviation of the 1000 estimates.\n",
    "\n",
    "4. Look at some of the results. Ratio to exact is computed as well to gain more understanding of convergence.\n",
    "\n",
    "5. Plot results, part 1. The first set of 6 plots will compare the estimates of the two methods for the mean, MPE and Qual levels for both the 1.5dB and 3.0dB standard deviations.\n",
    "\n",
    "6. Plot results, part 2. The next plot will compare the variation of the Qual level estimate from both the 1.5dB and 3.0dB standard deviations.\n",
    "    \n",
    "7. Draw some conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define analysis parameters\n",
    "geomean = 10.0\n",
    "mu = np.log10(geomean)\n",
    "\n",
    "sample_sizes = np.r_[2:10, 10_000]\n",
    "n_simulations = 1000\n",
    "\n",
    "stddevs = [1.5, 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate a pandas DataFrame to store simulation results\n",
    "\n",
    "# lowest level labels chosen like this for nice legend labels:\n",
    "def Set_lbl(stddev):\n",
    "    return f\"Set Ïƒ (Ïƒ={stddev}dB)\"\n",
    "\n",
    "\n",
    "def NTL_lbl(stddev):\n",
    "    return f\"NTL (Ïƒ={stddev}dB)\"\n",
    "\n",
    "\n",
    "col_labels = []\n",
    "for stddev in stddevs:\n",
    "    col_labels.append(Set_lbl(stddev))\n",
    "    col_labels.append(NTL_lbl(stddev))\n",
    "\n",
    "cols = pd.MultiIndex.from_product(\n",
    "    [[\"Mean\", \"MPE\", \"Qual\"], [\"Level Estimate\", \"Estimate StdDev\"], col_labels],\n",
    "    names=[\"Limit\", \"Statistic\", \"Method\"],\n",
    ")\n",
    "df = pd.DataFrame(index=sample_sizes, columns=cols)\n",
    "df.index.name = \"Number of Samples\"\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first 8 columns (the \"Mean\" section) to see layout:\n",
    "df.iloc[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run simulation to fill DataFrame with results:\n",
    "exact = {}\n",
    "for stddev in stddevs:\n",
    "    sigma = stddev / 20  # log10 sigma\n",
    "    exact[stddev] = {}\n",
    "\n",
    "    for n in sample_sizes:\n",
    "        for level, (p, c) in levels.items():\n",
    "            method1 = np.empty(n_simulations)\n",
    "            method2 = np.empty(n_simulations)\n",
    "            zp = norm.ppf(p)  # 1.64 for p == 0.95\n",
    "            zc = norm.ppf(c)  # 0.0 for c == 0.50\n",
    "            k1 = zp + zc / np.sqrt(n)\n",
    "            k2 = pyyeti.stats.ksingle(p, c, n)\n",
    "            exact[stddev][level] = 10 ** (mu + zp * (stddev / 20))\n",
    "            for i in range(n_simulations):\n",
    "                # generate lognormal deviates; here are two ways:\n",
    "                # X_flight = 10 ** (sigma * np.random.randn(n) + mu)\n",
    "                # X_flight = lognorm.rvs(size=n, s=np.log(10)*sigma, scale=geomean)\n",
    "                X_flight = lognorm.rvs(size=n, s=np.log(10)*sigma, scale=geomean)\n",
    "                Y = np.log10(X_flight)\n",
    "                m = Y.mean()\n",
    "                s = Y.std(ddof=1)  # should ~= sigma = stddev / 20\n",
    "                method1[i] = 10 ** (m + k1 * (stddev / 20))\n",
    "                method2[i] = 10 ** (m + k2 * s)\n",
    "            df.loc[\n",
    "                n, (level, [\"Level Estimate\", \"Estimate StdDev\"], Set_lbl(stddev))\n",
    "            ] = (\n",
    "                method1.mean(),\n",
    "                method1.std(ddof=1),\n",
    "            )\n",
    "            df.loc[\n",
    "                n, (level, [\"Level Estimate\", \"Estimate StdDev\"], NTL_lbl(stddev))\n",
    "            ] = (\n",
    "                method2.mean(),\n",
    "                method2.std(ddof=1),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Look at some of the results:\n",
    "df['Mean']['Level Estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MPE']['Level Estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Qual']['Level Estimate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at exact levels and then compute some ratios:\n",
    "exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPE, Ïƒ=1.5dB:\n",
    "df['MPE']['Level Estimate'][['Set Ïƒ (Ïƒ=1.5dB)', 'NTL (Ïƒ=1.5dB)']] / exact[1.5]['MPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPE, Ïƒ=3.0dB:\n",
    "df['MPE']['Level Estimate'][['Set Ïƒ (Ïƒ=3.0dB)', 'NTL (Ïƒ=3.0dB)']] / exact[3.0]['MPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qual, Ïƒ=1.5dB:\n",
    "q15_to_exact = df['Qual']['Level Estimate'][['Set Ïƒ (Ïƒ=1.5dB)', 'NTL (Ïƒ=1.5dB)']] / exact[1.5]['Qual']\n",
    "q15_to_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qual, Ïƒ=3.0dB:\n",
    "q30_to_exact = df['Qual']['Level Estimate'][['Set Ïƒ (Ïƒ=3.0dB)', 'NTL (Ïƒ=3.0dB)']] / exact[3.0]['Qual']\n",
    "q30_to_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot results, part 1:\n",
    "for stddev in stddevs:\n",
    "    fig = plt.figure(figsize=(10, 4.8))\n",
    "    axes = fig.subplots(1, 3)\n",
    "    for ax, (level, (p, c)) in zip(axes, levels.items()):\n",
    "        df[level][\"Level Estimate\"].plot.bar(\n",
    "            ax=ax,\n",
    "            y=[Set_lbl(stddev), NTL_lbl(stddev)],\n",
    "            yerr=df[level][\"Estimate StdDev\"],\n",
    "            capsize=4,\n",
    "            rot=0,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{level} (P{p*100:g}/{c*100:g}) - Ïƒ={stddev}dB \")\n",
    "        ax.set_ylabel(\"Level\")\n",
    "        if ax.get_ylim()[1] > 200.0:\n",
    "            ax.set_ylim(0, 200)\n",
    "\n",
    "        ax.axhline(\n",
    "            exact[stddev][level],\n",
    "            color=\"black\",\n",
    "            alpha=0.5,\n",
    "            linewidth=4,\n",
    "            label=\"Exact\",\n",
    "            zorder=-1,\n",
    "        )\n",
    "\n",
    "        ax.legend()\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot results, part 2:\n",
    "level = \"Qual\"\n",
    "p, c = levels[level]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1, 1)\n",
    "df[level][\"Level Estimate\"].plot.bar(\n",
    "    ax=ax,\n",
    "    y=[NTL_lbl(3.0), NTL_lbl(1.5)],\n",
    "    yerr=df[level][\"Estimate StdDev\"],\n",
    "    capsize=4,\n",
    "    rot=0,\n",
    "    color=[\"steelblue\", \"orchid\"],\n",
    ")\n",
    "\n",
    "ax.set_title(f\"{level} (P{p*100:g}/{c*100:g}) - Comparison of Estimate Variation\")\n",
    "ax.set_ylabel(\"Level\")\n",
    "if ax.get_ylim()[1] > 200.0:\n",
    "    ax.set_ylim(0, 200)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Draw some conclusions:\n",
    "\n",
    "When using normal tolerance limits (NTL), there is a lot less variation in the results when estimating the mean or MPE levels than there is when estimating the Qual level. In other words, it takes significantly more samples to accurately estimate the Qual level than the MPE or mean levels. This shouldn't be too surprising: the Qual level is in the thinner part of the tail where data is naturally harder to come by. Requiring 90% confidence (meaning the estimate will be high 90% of the time) in this region has a large impact on the k-factor. For example, consider the extreme case of just two samples:\n",
    "\n",
    "| Statistical Level | k-factor (from ``pyyeti.stats.ksingle``) |\n",
    "| --- | --- |\n",
    "| P95/50 | 2.339 |\n",
    "| P99/50 | 3.376 |\n",
    "| P95/90 | 13.09 |\n",
    "| P99/90 | 18.50 |\n",
    "\n",
    "From the above results, 2 flights seemed sufficient to get a reasonable estimate of the mean. For MPE, even if the standard deviation is as high as 3 dB, 5 flights was enough to bring the average estimate to within 5%. For Qual however, with a standard deviation of 3 dB, 9 flights only brought the average estimate to within about 60%. From other runs, it took approximately 20 flights to get within 25% (on average). If the standard deviation was 1.5 dB, 9 flights brought the average Qual estimate to within 25%.\n",
    "\n",
    "It is interesting to look at the rate of improvement by adding samples. From the Qual plots and tables shown above, it is clear that adding a sample when there are very few samples helps significantly. However, the rate of improvement drops off quickly: the major gains by adding one sample have already taken place by ~6 samples for 1.5 dB and by ~9 samples for 3.0 dB. After that, significant gains require more and more samples. Here is a plot showing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = q15_to_exact.iloc[:-1].plot.line()\n",
    "q30_to_exact.iloc[:-1].plot.line(ax=ax)\n",
    "ax.set_ylim(1, 5)\n",
    "ax.set_ylabel(\"Estimate / Exact\")\n",
    "ax.set_title(\"Convergence of Qual \")\n",
    "ax.set_title(f\"Convergence for {level} (P{p*100:g}/{c*100:g}) level\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
